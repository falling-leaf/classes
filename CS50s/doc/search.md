## class 1: search

### basic approach

维护一个frontier, 记录当前可行的所有状态（state）

- 最初，frontier只有起始状态
- 如果frontier为空，则搜索失败（无结果）
- 对frontier中的每个状态，依次进行以下操作：
    - 如果该状态是目标状态（通过了goal test），则搜索成功（找到结果）
    - 否则，对该状态的所有可行操作（action）进行尝试，并将结果状态加入frontier  
- 重复上述过程，直到frontier为空或搜索成功

维护frontier的方法：

- stack：先进后出，直接指向深度优先搜索（DFS）
    - 找到结果时不一定最优，但步数符合逻辑
- queue：先进先出，直接指向广度优先搜索（BFS）
    - 找到结果时一定最优，但步数不一定符合逻辑（例如多个路线同时并行，并不是一种实际方法）

问题：如果存在环，可能出现无限循环

### revised approach

在basic approach的基础上，设置一个visited set，记录所有已经访问过的状态，避免重复搜索

### informed search

上述BFS/DFS是uninformed search，即没有考虑到启发式信息。启发式信息可以帮助搜索算法更快地找到最优解。

可以利用某些辅助信息，进一步确认搜索顺序

即：使用一种启发式函数（heuristic function），来作为选择的决策

#### greedy best-first search

GBFS: 优先选择距离goal最近的状态

> 实际上，在找到goal之前是没有办法知道距离goal最近的状态的

对于迷宫等问题，具备更多的问题信息（例如整个迷宫的地图），可以利用启发式信息来加速搜索

（例如goal和state的直线位置）作为cost比较

- 这种方法中的heuristic function就是计算goal和state的manhattan距离

manhattan距离：

g(x1, y1); s(x2, y2), 则g与s的曼哈顿距离为|x1-x2|+|y1-y2|

问题：这种方法事实上不一定能够找到最优解。具体问题在于，h(x)函数不够完善

#### A* search

A* search: 结合了启发式信息和路径长度

使用g(n) + h(n)计算代价

g(n): 到达某个状态的代价
h(n): 从当前状态到目标状态的估计代价

A* 算法需要保证如下内容，才能够确定其结果是最优的：

- 1. h(n) is admissible(估计值永远不高于实际代价)
- 2. h(n) is consistent(对于任一节点n花费代价c到下一个节点n', h(n) <= h(n') + c)

举例：迷宫中的曼哈顿距离：

- 1. h(n)必然小于实际代价（曼哈顿距离就是在没有任何障碍物时的最佳路线）
- 2. h(n)是一致的，不可能存在某个节点，花费了代价但到达了h(x)更小的位置

### minimax/alpha-beta pruning

在更多智能体的情况下（例如二人的对战游戏），实际的策略会有所不同

在minimax算法中，将多个智能体的操作分解为：

- 最大化自己能得到的分
- 最小化对手能得到的分

那么，当一个智能体进行规划时：

- 当为自己的回合时：取所有子可能中分数最高的
- 当为对手的回合时：取所有子可能中分数最低的

这意味着，在游戏一开始时，最坏的情况需要计算所有的可能性，才能够下出第一步。

- 也就是说，一开始可供的选择越多，算法的运行时间越长

alpha-beta pruning: 减少搜索树的大小

基于minimax的思想：

- 如果是自己的回合，且已知有一种可能，对方能取到最小值为4
- 当计算另一种可能的对方选择时，如果对方能取到的最小值小于4，这意味着这种可能的最终分数不会超过4，因此可以舍弃

### depth-limited minimax

在minimax算法中，每一步都需要计算所有可能的结果，因此，当搜索树很大时，计算时间会很长。

因此，尝试采用depth-limited minimax算法，即限制搜索树的深度，当搜索树达到一定深度时，停止继续搜索。

这种方法的问题在于，在无法得知结果的情况下，如何评价当前的局势并给出分数

- 这种评价函数叫做evaluation function，需要根据具体问题进行设计
- 例如国际象棋中，假设不同棋子存活都有一定分数


